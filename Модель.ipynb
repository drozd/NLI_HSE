{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем все нужные нам функции из пакетов pandas, scikit-learn и time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, confusion_matrix, accuracy_score, confusion_matrix \n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем функцию pipeline_fit, которой передаем параметры corpus (объект нашего корпуса), is_binary (использователь бинарное или частотное представление), ngram_range (какие n-граммы использовать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pipeline_fit(corpus, is_binary=False, ngram_range=(1,1), is_linear=True):\n",
    "    norm = None\n",
    "    if is_linear:\n",
    "        norm = u'l2'\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(use_idf=is_linear, norm=norm, analyzer=\"word\", ngram_range=ngram_range, max_features=100000000, binary=is_binary)),\n",
    "        ('clf', LinearSVC())\n",
    "    ]) # \n",
    "    pipeline.fit(corpus.data, corpus.lang) # обучаем\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем отдельную функцию, которая принимает в качестве аргументов название папки и файла и возвращает содержимое этого файла "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filename_to_text(folder, filename):\n",
    "    PATH = folder + \"/tokenized/\"\n",
    "    f = open(PATH + filename, \"r\")\n",
    "    return \" \".join([l[:-1] for l in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем файл с корпусом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_sample_train = pd.read_csv(\"train/index-training.csv\", header=None, names=(\"filename\", \"type\", \"lang\", \"level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку корпус индексированный, то есть в столбце filename лежат названия файлов с текстами, а не сами тексты, нам нужно сделать столбец именно с текстами. Делаем это так: к каждой ячейке столбца filename применяем функцию filename_to_text, обернув ее в лямбду просто для того, чтобы мы могли передать этой функции папку с текстами (типа просто забиндили аргументы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_sample_train['data'] = corpus_sample_train['filename'].apply(lambda x : filename_to_text(\"train\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто массивчики с вариантами конфигурации наших моделек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_variants = [True, False]\n",
    "linear_variants = [True, False]\n",
    "ngram_variants = [(1,1), (2,2), (1,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой комбинации вариантов обучим модель и добавим результат обучения в массив res, заодно замеряем и выводим время работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=True\n",
      "\n",
      "2.76016801986s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=True\n",
      "\n",
      "8.34278132099s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=True\n",
      "\n",
      "11.1113102223s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=False\n",
      "\n",
      "3.71544388296s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=False\n",
      "\n",
      "8.77536987799s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=False\n",
      "\n",
      "11.9964737392s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=True\n",
      "\n",
      "2.86539990051s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=True\n",
      "\n",
      "8.30669322732s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=True\n",
      "\n",
      "11.3656639485s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=False\n",
      "\n",
      "11.5130949671s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=False\n",
      "\n",
      "9.89123138347s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=False\n",
      "\n",
      "16.5338250988s elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for bv in binary_variants:\n",
    "    for lv in linear_variants:\n",
    "        for nv in ngram_variants:\n",
    "            start_time = clock()\n",
    "            res.append((bv, nv, lv, pipeline_fit(corpus_sample_train, bv, nv, lv)))\n",
    "            print(\"\\n\\nConfiguration: binary=\" + str(bv) +\"; ngram_range=(\" + (\",\".join([str(x) for x in nv])) + \"), linear=\" + str(lv) + \"\\n\")\n",
    "            print(str(clock() - start_time) + \"s elapsed\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем тестовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_sample_test = pd.read_csv(\"test/index-dev.csv\", header=None, names=(\"filename\", \"type\", \"lang\", \"level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с корпусом, на котором мы обучали модель, тестовые данные индексированные. Делаем то же самое, но, разумеется, изменяем папку с текстовыми файлами в лямбда-функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_sample_test['data'] = corpus_sample_test['filename'].apply(lambda x : filename_to_text(\"test\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем результаты на тестовых данных и выводим precision и accuracy для каждой из моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.84      0.77      0.80       100\n",
      "        CHI       0.70      0.74      0.72       100\n",
      "        FRE       0.81      0.85      0.83       100\n",
      "        GER       0.78      0.91      0.84       100\n",
      "        HIN       0.64      0.69      0.66       100\n",
      "        ITA       0.87      0.85      0.86       100\n",
      "        JPN       0.72      0.69      0.70       100\n",
      "        KOR       0.77      0.70      0.73       100\n",
      "        SPA       0.81      0.71      0.76       100\n",
      "        TEL       0.69      0.71      0.70       100\n",
      "        TUR       0.81      0.79      0.80       100\n",
      "\n",
      "avg / total       0.77      0.76      0.76      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.764545454545\n",
      "[[77  3  3  2  4  0  2  1  2  3  3]\n",
      " [ 1 74  1  0  2  0  6  8  0  2  6]\n",
      " [ 0  2 85  6  2  1  2  0  2  0  0]\n",
      " [ 0  0  3 91  0  2  0  1  3  0  0]\n",
      " [ 1  2  1  1 69  0  0  1  1 23  1]\n",
      " [ 0  1  4  3  0 85  0  0  3  1  3]\n",
      " [ 4  8  1  2  2  2 69  8  2  0  2]\n",
      " [ 3  7  0  1  1  1 14 70  1  1  1]\n",
      " [ 5  2  4  5  1  5  3  1 71  0  3]\n",
      " [ 1  1  0  0 27  0  0  0  0 71  0]\n",
      " [ 0  5  3  5  0  2  0  1  3  2 79]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.90      0.74      0.81       100\n",
      "        CHI       0.83      0.88      0.85       100\n",
      "        FRE       0.88      0.83      0.86       100\n",
      "        GER       0.81      0.95      0.88       100\n",
      "        HIN       0.67      0.75      0.71       100\n",
      "        ITA       0.81      0.88      0.85       100\n",
      "        JPN       0.76      0.77      0.77       100\n",
      "        KOR       0.83      0.76      0.79       100\n",
      "        SPA       0.80      0.74      0.77       100\n",
      "        TEL       0.75      0.79      0.77       100\n",
      "        TUR       0.90      0.82      0.86       100\n",
      "\n",
      "avg / total       0.81      0.81      0.81      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.81\n",
      "[[74  1  3  0  2  3  2  3  6  2  4]\n",
      " [ 0 88  0  1  3  0  3  4  0  1  0]\n",
      " [ 2  1 83  3  2  5  2  0  2  0  0]\n",
      " [ 0  0  1 95  2  1  0  0  0  0  1]\n",
      " [ 0  0  0  1 75  0  0  0  1 23  0]\n",
      " [ 0  0  5  1  0 88  0  0  5  0  1]\n",
      " [ 2  7  0  2  2  0 77  8  2  0  0]\n",
      " [ 0  6  0  3  1  0 14 76  0  0  0]\n",
      " [ 1  1  2  5  1 11  2  0 74  0  3]\n",
      " [ 0  0  0  0 21  0  0  0  0 79  0]\n",
      " [ 3  2  0  6  3  0  1  1  2  0 82]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.93      0.78      0.85       100\n",
      "        CHI       0.85      0.88      0.86       100\n",
      "        FRE       0.88      0.88      0.88       100\n",
      "        GER       0.81      0.95      0.88       100\n",
      "        HIN       0.72      0.78      0.75       100\n",
      "        ITA       0.89      0.92      0.91       100\n",
      "        JPN       0.80      0.80      0.80       100\n",
      "        KOR       0.87      0.81      0.84       100\n",
      "        SPA       0.83      0.78      0.80       100\n",
      "        TEL       0.78      0.82      0.80       100\n",
      "        TUR       0.89      0.82      0.85       100\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.838181818182\n",
      "[[78  2  4  1  2  2  0  1  5  2  3]\n",
      " [ 0 88  0  0  4  1  3  3  0  1  0]\n",
      " [ 1  1 88  6  1  1  1  0  1  0  0]\n",
      " [ 0  0  2 95  0  0  0  0  2  0  1]\n",
      " [ 0  0  0  1 78  0  0  0  1 19  1]\n",
      " [ 0  0  2  2  0 92  1  0  3  0  0]\n",
      " [ 3  6  0  1  1  0 80  7  2  0  0]\n",
      " [ 0  5  0  2  0  0 12 81  0  0  0]\n",
      " [ 2  0  2  3  1  7  2  0 78  0  5]\n",
      " [ 0  0  0  0 18  0  0  0  0 82  0]\n",
      " [ 0  2  2  6  3  0  1  1  2  1 82]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.69      0.73      0.71       100\n",
      "        CHI       0.67      0.68      0.68       100\n",
      "        FRE       0.77      0.78      0.78       100\n",
      "        GER       0.79      0.81      0.80       100\n",
      "        HIN       0.61      0.61      0.61       100\n",
      "        ITA       0.75      0.77      0.76       100\n",
      "        JPN       0.70      0.67      0.68       100\n",
      "        KOR       0.65      0.69      0.67       100\n",
      "        SPA       0.72      0.63      0.67       100\n",
      "        TEL       0.70      0.67      0.68       100\n",
      "        TUR       0.68      0.69      0.68       100\n",
      "\n",
      "avg / total       0.70      0.70      0.70      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.702727272727\n",
      "[[73  2  2  1  5  3  1  3  4  2  4]\n",
      " [ 3 68  1  0  1  0  8  7  2  4  6]\n",
      " [ 2  3 78  5  1  5  1  0  3  0  2]\n",
      " [ 0  0  4 81  2  3  1  2  4  0  3]\n",
      " [ 4  3  2  2 61  1  1  2  2 21  1]\n",
      " [ 0  5  6  2  0 77  2  0  5  0  3]\n",
      " [ 5  7  0  0  1  2 67 14  1  0  3]\n",
      " [ 5  6  1  0  1  1 12 69  1  1  3]\n",
      " [ 8  2  4  3  2  9  2  1 63  0  6]\n",
      " [ 2  0  0  2 23  0  0  4  0 67  2]\n",
      " [ 4  5  3  7  3  1  1  4  2  1 69]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.71      0.73      0.72       100\n",
      "        CHI       0.76      0.84      0.80       100\n",
      "        FRE       0.87      0.74      0.80       100\n",
      "        GER       0.84      0.88      0.86       100\n",
      "        HIN       0.71      0.75      0.73       100\n",
      "        ITA       0.74      0.84      0.79       100\n",
      "        JPN       0.69      0.74      0.71       100\n",
      "        KOR       0.78      0.71      0.74       100\n",
      "        SPA       0.74      0.67      0.71       100\n",
      "        TEL       0.81      0.79      0.80       100\n",
      "        TUR       0.81      0.75      0.78       100\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.767272727273\n",
      "[[73  3  2  0  3  5  3  2  7  0  2]\n",
      " [ 1 84  1  0  1  1  3  4  0  1  4]\n",
      " [ 3  2 74  3  2  7  1  0  2  0  6]\n",
      " [ 1  1  2 88  3  2  0  0  1  0  2]\n",
      " [ 3  0  0  2 75  1  0  0  2 17  0]\n",
      " [ 2  1  4  0  1 84  2  0  5  0  1]\n",
      " [ 6  5  0  2  1  0 74 10  2  0  0]\n",
      " [ 0  7  1  2  1  0 17 71  1  0  0]\n",
      " [ 4  4  1  2  1 11  5  2 67  0  3]\n",
      " [ 3  0  0  1 16  1  0  0  0 79  0]\n",
      " [ 7  3  0  5  1  1  3  2  3  0 75]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.81      0.80      0.80       100\n",
      "        CHI       0.78      0.83      0.80       100\n",
      "        FRE       0.85      0.79      0.82       100\n",
      "        GER       0.83      0.91      0.87       100\n",
      "        HIN       0.74      0.78      0.76       100\n",
      "        ITA       0.79      0.87      0.83       100\n",
      "        JPN       0.74      0.73      0.74       100\n",
      "        KOR       0.77      0.79      0.78       100\n",
      "        SPA       0.85      0.72      0.78       100\n",
      "        TEL       0.79      0.78      0.78       100\n",
      "        TUR       0.84      0.76      0.80       100\n",
      "\n",
      "avg / total       0.80      0.80      0.80      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.796363636364\n",
      "[[80  4  3  0  0  4  4  0  2  1  2]\n",
      " [ 2 83  0  0  4  1  2  5  0  1  2]\n",
      " [ 1  4 79  5  2  4  1  1  1  0  2]\n",
      " [ 0  0  1 91  1  4  0  0  1  1  1]\n",
      " [ 1  0  1  1 78  0  0  1  1 17  0]\n",
      " [ 1  1  5  1  0 87  1  0  3  0  1]\n",
      " [ 4  5  0  1  0  0 73 14  1  1  1]\n",
      " [ 1  4  1  2  0  0 12 79  0  0  1]\n",
      " [ 6  2  2  4  0  7  3  0 72  0  4]\n",
      " [ 1  0  0  0 20  0  0  0  0 78  1]\n",
      " [ 2  4  1  5  1  3  2  2  4  0 76]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.79      0.69      0.74       100\n",
      "        CHI       0.74      0.75      0.75       100\n",
      "        FRE       0.81      0.82      0.82       100\n",
      "        GER       0.76      0.87      0.81       100\n",
      "        HIN       0.65      0.68      0.67       100\n",
      "        ITA       0.88      0.85      0.86       100\n",
      "        JPN       0.79      0.74      0.76       100\n",
      "        KOR       0.78      0.76      0.77       100\n",
      "        SPA       0.76      0.65      0.70       100\n",
      "        TEL       0.67      0.73      0.70       100\n",
      "        TUR       0.72      0.80      0.76       100\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.758181818182\n",
      "[[69  4  2  4  4  1  3  1  4  5  3]\n",
      " [ 1 75  2  2  2  1  1  7  2  2  5]\n",
      " [ 3  3 82  6  1  1  1  0  1  0  2]\n",
      " [ 0  0  1 87  1  1  1  1  3  0  5]\n",
      " [ 0  0  2  0 68  0  0  0  2 26  2]\n",
      " [ 1  0  5  2  0 85  0  1  2  1  3]\n",
      " [ 1 10  0  3  1  2 74  8  1  0  0]\n",
      " [ 4  5  2  0  0  0 10 76  0  0  3]\n",
      " [ 6  2  3  5  1  6  3  1 65  1  7]\n",
      " [ 1  0  0  1 24  0  0  0  0 73  1]\n",
      " [ 1  2  2  4  2  0  1  2  5  1 80]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.88      0.72      0.79       100\n",
      "        CHI       0.80      0.82      0.81       100\n",
      "        FRE       0.89      0.84      0.87       100\n",
      "        GER       0.80      0.91      0.85       100\n",
      "        HIN       0.67      0.70      0.68       100\n",
      "        ITA       0.81      0.87      0.84       100\n",
      "        JPN       0.81      0.77      0.79       100\n",
      "        KOR       0.73      0.76      0.75       100\n",
      "        SPA       0.76      0.70      0.73       100\n",
      "        TEL       0.72      0.77      0.74       100\n",
      "        TUR       0.82      0.80      0.81       100\n",
      "\n",
      "avg / total       0.79      0.79      0.79      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.787272727273\n",
      "[[72  1  3  0  2  5  1  5  6  2  3]\n",
      " [ 0 82  1  1  2  0  3  8  1  0  2]\n",
      " [ 3  1 84  4  1  3  0  1  2  0  1]\n",
      " [ 0  0  1 91  2  2  0  0  0  1  3]\n",
      " [ 0  1  0  1 70  0  0  0  2 26  0]\n",
      " [ 0  0  3  2  0 87  1  0  6  0  1]\n",
      " [ 3  7  1  2  1  0 77  8  1  0  0]\n",
      " [ 0  7  0  3  2  0 10 76  0  0  2]\n",
      " [ 2  3  1  5  0  9  2  3 70  0  5]\n",
      " [ 0  0  0  0 22  0  0  1  0 77  0]\n",
      " [ 2  1  0  5  3  1  1  2  4  1 80]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.89      0.71      0.79       100\n",
      "        CHI       0.82      0.84      0.83       100\n",
      "        FRE       0.87      0.85      0.86       100\n",
      "        GER       0.78      0.93      0.85       100\n",
      "        HIN       0.65      0.71      0.68       100\n",
      "        ITA       0.89      0.85      0.87       100\n",
      "        JPN       0.83      0.81      0.82       100\n",
      "        KOR       0.84      0.87      0.85       100\n",
      "        SPA       0.80      0.76      0.78       100\n",
      "        TEL       0.73      0.75      0.74       100\n",
      "        TUR       0.84      0.80      0.82       100\n",
      "\n",
      "avg / total       0.81      0.81      0.81      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.807272727273\n",
      "[[71  4  3  1  2  3  2  4  4  4  2]\n",
      " [ 1 84  0  2  2  0  2  6  1  0  2]\n",
      " [ 2  1 85  6  2  1  1  0  1  0  1]\n",
      " [ 0  0  1 93  2  0  0  0  1  0  3]\n",
      " [ 0  0  0  1 71  0  1  0  3 23  1]\n",
      " [ 1  1  5  2  0 85  1  0  3  0  2]\n",
      " [ 1  6  0  3  1  0 81  6  2  0  0]\n",
      " [ 0  3  0  2  0  0  8 87  0  0  0]\n",
      " [ 3  2  2  4  1  7  1  0 76  0  4]\n",
      " [ 0  0  0  0 25  0  0  0  0 75  0]\n",
      " [ 1  2  2  5  3  0  1  1  4  1 80]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.74      0.65      0.69       100\n",
      "        CHI       0.68      0.69      0.68       100\n",
      "        FRE       0.72      0.79      0.75       100\n",
      "        GER       0.74      0.75      0.74       100\n",
      "        HIN       0.63      0.55      0.59       100\n",
      "        ITA       0.73      0.83      0.78       100\n",
      "        JPN       0.69      0.66      0.68       100\n",
      "        KOR       0.65      0.67      0.66       100\n",
      "        SPA       0.67      0.56      0.61       100\n",
      "        TEL       0.66      0.74      0.70       100\n",
      "        TUR       0.70      0.73      0.71       100\n",
      "\n",
      "avg / total       0.69      0.69      0.69      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.692727272727\n",
      "[[65  5  5  3  2  2  6  3  2  4  3]\n",
      " [ 1 69  1  1  3  3  3  8  2  3  6]\n",
      " [ 2  2 79  5  0  4  2  2  3  0  1]\n",
      " [ 0  3  3 75  1  5  1  2  6  0  4]\n",
      " [ 2  0  4  4 55  1  1  1  5 26  1]\n",
      " [ 0  0  5  4  1 83  1  2  2  0  2]\n",
      " [ 2 12  0  0  0  3 66 14  1  0  2]\n",
      " [ 4  6  3  1  2  0 11 67  1  2  3]\n",
      " [ 7  2  7  3  3  9  4  0 56  2  7]\n",
      " [ 0  1  0  2 18  0  0  2  0 74  3]\n",
      " [ 5  2  3  4  2  3  0  2  5  1 73]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.70      0.70      0.70       100\n",
      "        CHI       0.74      0.75      0.75       100\n",
      "        FRE       0.84      0.80      0.82       100\n",
      "        GER       0.82      0.86      0.84       100\n",
      "        HIN       0.67      0.71      0.69       100\n",
      "        ITA       0.76      0.86      0.81       100\n",
      "        JPN       0.69      0.73      0.71       100\n",
      "        KOR       0.71      0.72      0.71       100\n",
      "        SPA       0.72      0.63      0.67       100\n",
      "        TEL       0.78      0.73      0.75       100\n",
      "        TUR       0.75      0.68      0.71       100\n",
      "\n",
      "avg / total       0.74      0.74      0.74      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.742727272727\n",
      "[[70  2  3  0  1  6  4  5  5  1  3]\n",
      " [ 1 75  2  2  2  0  6  8  1  0  3]\n",
      " [ 2  1 80  3  2  4  1  0  4  0  3]\n",
      " [ 1  0  2 86  4  2  0  0  1  1  3]\n",
      " [ 4  0  2  1 71  2  1  0  1 17  1]\n",
      " [ 1  0  2  2  0 86  1  0  6  1  1]\n",
      " [ 6 10  0  1  0  0 73  9  1  0  0]\n",
      " [ 0  8  0  3  1  0 14 72  1  0  1]\n",
      " [ 4  3  2  2  2 10  3  4 63  0  7]\n",
      " [ 2  1  0  0 21  1  1  0  0 73  1]\n",
      " [ 9  1  2  5  2  2  2  4  4  1 68]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.75      0.76      0.75       100\n",
      "        CHI       0.75      0.72      0.73       100\n",
      "        FRE       0.84      0.81      0.82       100\n",
      "        GER       0.81      0.87      0.84       100\n",
      "        HIN       0.65      0.65      0.65       100\n",
      "        ITA       0.85      0.82      0.83       100\n",
      "        JPN       0.79      0.77      0.78       100\n",
      "        KOR       0.76      0.80      0.78       100\n",
      "        SPA       0.74      0.65      0.69       100\n",
      "        TEL       0.75      0.76      0.76       100\n",
      "        TUR       0.72      0.79      0.76       100\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.763636363636\n",
      "[[76  4  3  0  2  2  3  2  5  0  3]\n",
      " [ 2 72  0  2  3  1  4 10  1  0  5]\n",
      " [ 4  2 81  3  2  1  1  0  2  0  4]\n",
      " [ 1  1  1 87  1  2  0  1  1  1  4]\n",
      " [ 2  0  1  1 65  2  1  1  2 23  2]\n",
      " [ 1  1  4  4  0 82  2  0  5  0  1]\n",
      " [ 2  7  0  1  1  1 77  8  2  0  1]\n",
      " [ 0  5  2  3  1  0  6 80  1  0  2]\n",
      " [ 9  2  3  3  1  5  3  2 65  0  7]\n",
      " [ 1  0  0  0 21  1  0  0  0 76  1]\n",
      " [ 4  2  2  4  3  0  0  1  4  1 79]]\n"
     ]
    }
   ],
   "source": [
    "for result in res:\n",
    "    \n",
    "    predictions = result[3].predict(corpus_sample_test.data)\n",
    "    print(\"\\n\\nConfiguration: binary=\" + str(result[0]) +\"; ngram_range=(\" + (\",\".join([str(x) for x in result[1]])) + \"), linear=\" + str(result[2]) + \"\\n\")\n",
    "    print(classification_report(corpus_sample_test.lang, predictions))\n",
    "    print(\"\\nAccuracy: \" + str(accuracy_score(corpus_sample_test.lang, predictions)))\n",
    "    print(confusion_matrix(corpus_sample_test.lang, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас мы проделаем то же самое, но добавим функцию, которая будет убирать знаки препинания из текстов, и обучим и протестируем наши модели уже на текстах без них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    marks = set(\"!\\\"#$%&')*+,-./:;<=>?@[\\]^_`{|}~\")\n",
    "    marks.add(\"--\")\n",
    "    return \" \".join([word for word in s.split() if word not in marks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_sample_train['data'] = corpus_sample_train['filename'].apply(lambda x : remove_punctuation(filename_to_text(\"train\", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_sample_test['data'] = corpus_sample_test['filename'].apply(lambda x : remove_punctuation(filename_to_text(\"test\", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=True\n",
      "\n",
      "2.80748741524s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=True\n",
      "\n",
      "8.50427829306s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=True\n",
      "\n",
      "11.6657336204s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=False\n",
      "\n",
      "3.76195669919s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=False\n",
      "\n",
      "9.25020075058s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=False\n",
      "\n",
      "12.4442168888s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=True\n",
      "\n",
      "3.00805889331s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=True\n",
      "\n",
      "8.79339182501s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=True\n",
      "\n",
      "11.4562856838s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=False\n",
      "\n",
      "10.7722087111s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=False\n",
      "\n",
      "9.57161960429s elapsed\n",
      "\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=False\n",
      "\n",
      "16.1390755024s elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_no_punc = []\n",
    "for bv in binary_variants:\n",
    "    for lv in linear_variants:\n",
    "        for nv in ngram_variants:\n",
    "            start_time = clock()\n",
    "            res_no_punc.append((bv, nv, lv, pipeline_fit(corpus_sample_train, bv, nv, lv)))\n",
    "            print(\"\\n\\nConfiguration: binary=\" + str(bv) +\"; ngram_range=(\" + (\",\".join([str(x) for x in nv])) + \"), linear=\" + str(lv) + \"\\n\")\n",
    "            print(str(clock() - start_time) + \"s elapsed\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.84      0.77      0.80       100\n",
      "        CHI       0.70      0.74      0.72       100\n",
      "        FRE       0.81      0.85      0.83       100\n",
      "        GER       0.78      0.91      0.84       100\n",
      "        HIN       0.64      0.69      0.66       100\n",
      "        ITA       0.87      0.85      0.86       100\n",
      "        JPN       0.72      0.69      0.70       100\n",
      "        KOR       0.77      0.70      0.73       100\n",
      "        SPA       0.81      0.71      0.76       100\n",
      "        TEL       0.69      0.71      0.70       100\n",
      "        TUR       0.81      0.79      0.80       100\n",
      "\n",
      "avg / total       0.77      0.76      0.76      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.764545454545\n",
      "[[77  3  3  2  4  0  2  1  2  3  3]\n",
      " [ 1 74  1  0  2  0  6  8  0  2  6]\n",
      " [ 0  2 85  6  2  1  2  0  2  0  0]\n",
      " [ 0  0  3 91  0  2  0  1  3  0  0]\n",
      " [ 1  2  1  1 69  0  0  1  1 23  1]\n",
      " [ 0  1  4  3  0 85  0  0  3  1  3]\n",
      " [ 4  8  1  2  2  2 69  8  2  0  2]\n",
      " [ 3  7  0  1  1  1 14 70  1  1  1]\n",
      " [ 5  2  4  5  1  5  3  1 71  0  3]\n",
      " [ 1  1  0  0 27  0  0  0  0 71  0]\n",
      " [ 0  5  3  5  0  2  0  1  3  2 79]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.90      0.74      0.81       100\n",
      "        CHI       0.83      0.88      0.85       100\n",
      "        FRE       0.88      0.83      0.86       100\n",
      "        GER       0.81      0.95      0.88       100\n",
      "        HIN       0.67      0.75      0.71       100\n",
      "        ITA       0.81      0.88      0.85       100\n",
      "        JPN       0.76      0.77      0.77       100\n",
      "        KOR       0.83      0.76      0.79       100\n",
      "        SPA       0.80      0.74      0.77       100\n",
      "        TEL       0.75      0.79      0.77       100\n",
      "        TUR       0.90      0.82      0.86       100\n",
      "\n",
      "avg / total       0.81      0.81      0.81      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.81\n",
      "[[74  1  3  0  2  3  2  3  6  2  4]\n",
      " [ 0 88  0  1  3  0  3  4  0  1  0]\n",
      " [ 2  1 83  3  2  5  2  0  2  0  0]\n",
      " [ 0  0  1 95  2  1  0  0  0  0  1]\n",
      " [ 0  0  0  1 75  0  0  0  1 23  0]\n",
      " [ 0  0  5  1  0 88  0  0  5  0  1]\n",
      " [ 2  7  0  2  2  0 77  8  2  0  0]\n",
      " [ 0  6  0  3  1  0 14 76  0  0  0]\n",
      " [ 1  1  2  5  1 11  2  0 74  0  3]\n",
      " [ 0  0  0  0 21  0  0  0  0 79  0]\n",
      " [ 3  2  0  6  3  0  1  1  2  0 82]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.93      0.78      0.85       100\n",
      "        CHI       0.85      0.88      0.86       100\n",
      "        FRE       0.88      0.88      0.88       100\n",
      "        GER       0.81      0.95      0.88       100\n",
      "        HIN       0.72      0.78      0.75       100\n",
      "        ITA       0.89      0.92      0.91       100\n",
      "        JPN       0.80      0.80      0.80       100\n",
      "        KOR       0.87      0.81      0.84       100\n",
      "        SPA       0.83      0.78      0.80       100\n",
      "        TEL       0.78      0.82      0.80       100\n",
      "        TUR       0.89      0.82      0.85       100\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.838181818182\n",
      "[[78  2  4  1  2  2  0  1  5  2  3]\n",
      " [ 0 88  0  0  4  1  3  3  0  1  0]\n",
      " [ 1  1 88  6  1  1  1  0  1  0  0]\n",
      " [ 0  0  2 95  0  0  0  0  2  0  1]\n",
      " [ 0  0  0  1 78  0  0  0  1 19  1]\n",
      " [ 0  0  2  2  0 92  1  0  3  0  0]\n",
      " [ 3  6  0  1  1  0 80  7  2  0  0]\n",
      " [ 0  5  0  2  0  0 12 81  0  0  0]\n",
      " [ 2  0  2  3  1  7  2  0 78  0  5]\n",
      " [ 0  0  0  0 18  0  0  0  0 82  0]\n",
      " [ 0  2  2  6  3  0  1  1  2  1 82]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,1), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.69      0.73      0.71       100\n",
      "        CHI       0.67      0.68      0.68       100\n",
      "        FRE       0.77      0.78      0.78       100\n",
      "        GER       0.79      0.81      0.80       100\n",
      "        HIN       0.61      0.61      0.61       100\n",
      "        ITA       0.75      0.77      0.76       100\n",
      "        JPN       0.70      0.67      0.68       100\n",
      "        KOR       0.65      0.69      0.67       100\n",
      "        SPA       0.72      0.63      0.67       100\n",
      "        TEL       0.70      0.67      0.68       100\n",
      "        TUR       0.68      0.69      0.68       100\n",
      "\n",
      "avg / total       0.70      0.70      0.70      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.702727272727\n",
      "[[73  2  2  1  5  3  1  3  4  2  4]\n",
      " [ 3 68  1  0  1  0  8  7  2  4  6]\n",
      " [ 2  3 78  5  1  5  1  0  3  0  2]\n",
      " [ 0  0  4 81  2  3  1  2  4  0  3]\n",
      " [ 4  3  2  2 61  1  1  2  2 21  1]\n",
      " [ 0  5  6  2  0 77  2  0  5  0  3]\n",
      " [ 5  7  0  0  1  2 67 14  1  0  3]\n",
      " [ 5  6  1  0  1  1 12 69  1  1  3]\n",
      " [ 8  2  4  3  2  9  2  1 63  0  6]\n",
      " [ 2  0  0  2 23  0  0  4  0 67  2]\n",
      " [ 4  5  3  7  3  1  1  4  2  1 69]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(2,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.71      0.73      0.72       100\n",
      "        CHI       0.76      0.84      0.80       100\n",
      "        FRE       0.87      0.74      0.80       100\n",
      "        GER       0.84      0.88      0.86       100\n",
      "        HIN       0.71      0.75      0.73       100\n",
      "        ITA       0.74      0.84      0.79       100\n",
      "        JPN       0.69      0.74      0.71       100\n",
      "        KOR       0.78      0.71      0.74       100\n",
      "        SPA       0.74      0.67      0.71       100\n",
      "        TEL       0.81      0.79      0.80       100\n",
      "        TUR       0.81      0.75      0.78       100\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.767272727273\n",
      "[[73  3  2  0  3  5  3  2  7  0  2]\n",
      " [ 1 84  1  0  1  1  3  4  0  1  4]\n",
      " [ 3  2 74  3  2  7  1  0  2  0  6]\n",
      " [ 1  1  2 88  3  2  0  0  1  0  2]\n",
      " [ 3  0  0  2 75  1  0  0  2 17  0]\n",
      " [ 2  1  4  0  1 84  2  0  5  0  1]\n",
      " [ 6  5  0  2  1  0 74 10  2  0  0]\n",
      " [ 0  7  1  2  1  0 17 71  1  0  0]\n",
      " [ 4  4  1  2  1 11  5  2 67  0  3]\n",
      " [ 3  0  0  1 16  1  0  0  0 79  0]\n",
      " [ 7  3  0  5  1  1  3  2  3  0 75]]\n",
      "\n",
      "\n",
      "Configuration: binary=True; ngram_range=(1,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.81      0.80      0.80       100\n",
      "        CHI       0.78      0.83      0.80       100\n",
      "        FRE       0.85      0.79      0.82       100\n",
      "        GER       0.83      0.91      0.87       100\n",
      "        HIN       0.74      0.78      0.76       100\n",
      "        ITA       0.79      0.87      0.83       100\n",
      "        JPN       0.74      0.73      0.74       100\n",
      "        KOR       0.77      0.79      0.78       100\n",
      "        SPA       0.85      0.72      0.78       100\n",
      "        TEL       0.79      0.78      0.78       100\n",
      "        TUR       0.84      0.76      0.80       100\n",
      "\n",
      "avg / total       0.80      0.80      0.80      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.796363636364\n",
      "[[80  4  3  0  0  4  4  0  2  1  2]\n",
      " [ 2 83  0  0  4  1  2  5  0  1  2]\n",
      " [ 1  4 79  5  2  4  1  1  1  0  2]\n",
      " [ 0  0  1 91  1  4  0  0  1  1  1]\n",
      " [ 1  0  1  1 78  0  0  1  1 17  0]\n",
      " [ 1  1  5  1  0 87  1  0  3  0  1]\n",
      " [ 4  5  0  1  0  0 73 14  1  1  1]\n",
      " [ 1  4  1  2  0  0 12 79  0  0  1]\n",
      " [ 6  2  2  4  0  7  3  0 72  0  4]\n",
      " [ 1  0  0  0 20  0  0  0  0 78  1]\n",
      " [ 2  4  1  5  1  3  2  2  4  0 76]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.79      0.69      0.74       100\n",
      "        CHI       0.74      0.75      0.75       100\n",
      "        FRE       0.81      0.82      0.82       100\n",
      "        GER       0.76      0.87      0.81       100\n",
      "        HIN       0.65      0.68      0.67       100\n",
      "        ITA       0.88      0.85      0.86       100\n",
      "        JPN       0.79      0.74      0.76       100\n",
      "        KOR       0.78      0.76      0.77       100\n",
      "        SPA       0.76      0.65      0.70       100\n",
      "        TEL       0.67      0.73      0.70       100\n",
      "        TUR       0.72      0.80      0.76       100\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.758181818182\n",
      "[[69  4  2  4  4  1  3  1  4  5  3]\n",
      " [ 1 75  2  2  2  1  1  7  2  2  5]\n",
      " [ 3  3 82  6  1  1  1  0  1  0  2]\n",
      " [ 0  0  1 87  1  1  1  1  3  0  5]\n",
      " [ 0  0  2  0 68  0  0  0  2 26  2]\n",
      " [ 1  0  5  2  0 85  0  1  2  1  3]\n",
      " [ 1 10  0  3  1  2 74  8  1  0  0]\n",
      " [ 4  5  2  0  0  0 10 76  0  0  3]\n",
      " [ 6  2  3  5  1  6  3  1 65  1  7]\n",
      " [ 1  0  0  1 24  0  0  0  0 73  1]\n",
      " [ 1  2  2  4  2  0  1  2  5  1 80]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.88      0.72      0.79       100\n",
      "        CHI       0.80      0.82      0.81       100\n",
      "        FRE       0.89      0.84      0.87       100\n",
      "        GER       0.80      0.91      0.85       100\n",
      "        HIN       0.67      0.70      0.68       100\n",
      "        ITA       0.81      0.87      0.84       100\n",
      "        JPN       0.81      0.77      0.79       100\n",
      "        KOR       0.73      0.76      0.75       100\n",
      "        SPA       0.76      0.70      0.73       100\n",
      "        TEL       0.72      0.77      0.74       100\n",
      "        TUR       0.82      0.80      0.81       100\n",
      "\n",
      "avg / total       0.79      0.79      0.79      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.787272727273\n",
      "[[72  1  3  0  2  5  1  5  6  2  3]\n",
      " [ 0 82  1  1  2  0  3  8  1  0  2]\n",
      " [ 3  1 84  4  1  3  0  1  2  0  1]\n",
      " [ 0  0  1 91  2  2  0  0  0  1  3]\n",
      " [ 0  1  0  1 70  0  0  0  2 26  0]\n",
      " [ 0  0  3  2  0 87  1  0  6  0  1]\n",
      " [ 3  7  1  2  1  0 77  8  1  0  0]\n",
      " [ 0  7  0  3  2  0 10 76  0  0  2]\n",
      " [ 2  3  1  5  0  9  2  3 70  0  5]\n",
      " [ 0  0  0  0 22  0  0  1  0 77  0]\n",
      " [ 2  1  0  5  3  1  1  2  4  1 80]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=True\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.89      0.71      0.79       100\n",
      "        CHI       0.82      0.84      0.83       100\n",
      "        FRE       0.87      0.85      0.86       100\n",
      "        GER       0.78      0.93      0.85       100\n",
      "        HIN       0.65      0.71      0.68       100\n",
      "        ITA       0.89      0.85      0.87       100\n",
      "        JPN       0.83      0.81      0.82       100\n",
      "        KOR       0.84      0.87      0.85       100\n",
      "        SPA       0.80      0.76      0.78       100\n",
      "        TEL       0.73      0.75      0.74       100\n",
      "        TUR       0.84      0.80      0.82       100\n",
      "\n",
      "avg / total       0.81      0.81      0.81      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.807272727273\n",
      "[[71  4  3  1  2  3  2  4  4  4  2]\n",
      " [ 1 84  0  2  2  0  2  6  1  0  2]\n",
      " [ 2  1 85  6  2  1  1  0  1  0  1]\n",
      " [ 0  0  1 93  2  0  0  0  1  0  3]\n",
      " [ 0  0  0  1 71  0  1  0  3 23  1]\n",
      " [ 1  1  5  2  0 85  1  0  3  0  2]\n",
      " [ 1  6  0  3  1  0 81  6  2  0  0]\n",
      " [ 0  3  0  2  0  0  8 87  0  0  0]\n",
      " [ 3  2  2  4  1  7  1  0 76  0  4]\n",
      " [ 0  0  0  0 25  0  0  0  0 75  0]\n",
      " [ 1  2  2  5  3  0  1  1  4  1 80]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,1), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.74      0.65      0.69       100\n",
      "        CHI       0.68      0.69      0.68       100\n",
      "        FRE       0.72      0.79      0.75       100\n",
      "        GER       0.74      0.75      0.74       100\n",
      "        HIN       0.63      0.55      0.59       100\n",
      "        ITA       0.73      0.83      0.78       100\n",
      "        JPN       0.69      0.66      0.68       100\n",
      "        KOR       0.65      0.67      0.66       100\n",
      "        SPA       0.67      0.56      0.61       100\n",
      "        TEL       0.66      0.74      0.70       100\n",
      "        TUR       0.70      0.73      0.71       100\n",
      "\n",
      "avg / total       0.69      0.69      0.69      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.692727272727\n",
      "[[65  5  5  3  2  2  6  3  2  4  3]\n",
      " [ 1 69  1  1  3  3  3  8  2  3  6]\n",
      " [ 2  2 79  5  0  4  2  2  3  0  1]\n",
      " [ 0  3  3 75  1  5  1  2  6  0  4]\n",
      " [ 2  0  4  4 55  1  1  1  5 26  1]\n",
      " [ 0  0  5  4  1 83  1  2  2  0  2]\n",
      " [ 2 12  0  0  0  3 66 14  1  0  2]\n",
      " [ 4  6  3  1  2  0 11 67  1  2  3]\n",
      " [ 7  2  7  3  3  9  4  0 56  2  7]\n",
      " [ 0  1  0  2 18  0  0  2  0 74  3]\n",
      " [ 5  2  3  4  2  3  0  2  5  1 73]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(2,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.70      0.70      0.70       100\n",
      "        CHI       0.74      0.75      0.75       100\n",
      "        FRE       0.84      0.80      0.82       100\n",
      "        GER       0.82      0.86      0.84       100\n",
      "        HIN       0.67      0.71      0.69       100\n",
      "        ITA       0.76      0.86      0.81       100\n",
      "        JPN       0.69      0.73      0.71       100\n",
      "        KOR       0.71      0.72      0.71       100\n",
      "        SPA       0.72      0.63      0.67       100\n",
      "        TEL       0.78      0.73      0.75       100\n",
      "        TUR       0.75      0.68      0.71       100\n",
      "\n",
      "avg / total       0.74      0.74      0.74      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.742727272727\n",
      "[[70  2  3  0  1  6  4  5  5  1  3]\n",
      " [ 1 75  2  2  2  0  6  8  1  0  3]\n",
      " [ 2  1 80  3  2  4  1  0  4  0  3]\n",
      " [ 1  0  2 86  4  2  0  0  1  1  3]\n",
      " [ 4  0  2  1 71  2  1  0  1 17  1]\n",
      " [ 1  0  2  2  0 86  1  0  6  1  1]\n",
      " [ 6 10  0  1  0  0 73  9  1  0  0]\n",
      " [ 0  8  0  3  1  0 14 72  1  0  1]\n",
      " [ 4  3  2  2  2 10  3  4 63  0  7]\n",
      " [ 2  1  0  0 21  1  1  0  0 73  1]\n",
      " [ 9  1  2  5  2  2  2  4  4  1 68]]\n",
      "\n",
      "\n",
      "Configuration: binary=False; ngram_range=(1,2), linear=False\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ARA       0.75      0.76      0.75       100\n",
      "        CHI       0.75      0.72      0.73       100\n",
      "        FRE       0.84      0.81      0.82       100\n",
      "        GER       0.81      0.87      0.84       100\n",
      "        HIN       0.65      0.65      0.65       100\n",
      "        ITA       0.85      0.82      0.83       100\n",
      "        JPN       0.79      0.77      0.78       100\n",
      "        KOR       0.76      0.80      0.78       100\n",
      "        SPA       0.74      0.65      0.69       100\n",
      "        TEL       0.75      0.76      0.76       100\n",
      "        TUR       0.72      0.79      0.76       100\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1100\n",
      "\n",
      "\n",
      "Accuracy: 0.763636363636\n",
      "[[76  4  3  0  2  2  3  2  5  0  3]\n",
      " [ 2 72  0  2  3  1  4 10  1  0  5]\n",
      " [ 4  2 81  3  2  1  1  0  2  0  4]\n",
      " [ 1  1  1 87  1  2  0  1  1  1  4]\n",
      " [ 2  0  1  1 65  2  1  1  2 23  2]\n",
      " [ 1  1  4  4  0 82  2  0  5  0  1]\n",
      " [ 2  7  0  1  1  1 77  8  2  0  1]\n",
      " [ 0  5  2  3  1  0  6 80  1  0  2]\n",
      " [ 9  2  3  3  1  5  3  2 65  0  7]\n",
      " [ 1  0  0  0 21  1  0  0  0 76  1]\n",
      " [ 4  2  2  4  3  0  0  1  4  1 79]]\n"
     ]
    }
   ],
   "source": [
    "for result in res_no_punc:\n",
    "    \n",
    "    predictions = result[3].predict(corpus_sample_test.data)\n",
    "    print(\"\\n\\nConfiguration: binary=\" + str(result[0]) +\"; ngram_range=(\" + (\",\".join([str(x) for x in result[1]])) + \"), linear=\" + str(result[2]) + \"\\n\")\n",
    "    print(classification_report(corpus_sample_test.lang, predictions))\n",
    "    print(\"\\nAccuracy: \" + str(accuracy_score(corpus_sample_test.lang, predictions)))\n",
    "    print(confusion_matrix(corpus_sample_test.lang, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что со знаками препинания, что без них -- все одинаково"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
